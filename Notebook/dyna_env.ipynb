{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccf928fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c25c099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def __init__(self, input_size, output_size, num_hidden_layers, hidden_layer_nodes, activation, dropout_prob):\n",
    "        super(MC_Dropout_Net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.hidden_layer_nodes = hidden_layer_nodes\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Define the layers\n",
    "        self.input_layer = nn.Linear(input_size, hidden_layer_nodes)\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_layer_nodes, hidden_layer_nodes))\n",
    "            self.hidden_layers.append(nn.Dropout(p=dropout_prob))\n",
    "        self.output_layer = nn.Linear(hidden_layer_nodes, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation(hidden_layer(x))\n",
    "        output = self.output_layer(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87fb979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b163300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQ:\n",
    "    def __init__(self, state_dim, action_dim, learning_rate=0.001, gamma=0.99, epsilon=0.1, model_learning_rate=0.001, model_epochs=5):\n",
    "        self.q_network = QNetwork(state_dim, action_dim)\n",
    "        self.target_q_network = QNetwork(state_dim, action_dim)\n",
    "        self.model = Model(state_dim + action_dim, state_dim)\n",
    "        self.optimizer_q = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.optimizer_model = optim.Adam(self.model.parameters(), lr=model_learning_rate)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.model_epochs = model_epochs\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0, self.q_network.fc3.out_features)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(torch.tensor(state, dtype=torch.float32))\n",
    "                return torch.argmax(q_values).item()\n",
    "\n",
    "    def update(self, state, action, reward, next_state, terminal):\n",
    "        self.optimizer_q.zero_grad()\n",
    "        q_values = self.q_network(torch.tensor(state, dtype=torch.float32))\n",
    "        next_q_values = self.target_q_network(torch.tensor(next_state, dtype=torch.float32))\n",
    "        target_q = reward + (1 - terminal) * self.gamma * torch.max(next_q_values).item()\n",
    "        loss = nn.MSELoss()(q_values[action], target_q)\n",
    "        loss.backward()\n",
    "        self.optimizer_q.step()\n",
    "\n",
    "        # Update the model\n",
    "        self.optimizer_model.zero_grad()\n",
    "        input_model = torch.cat((torch.tensor(state, dtype=torch.float32), torch.tensor(action, dtype=torch.float32)))\n",
    "        predicted_next_state = self.model(input_model)\n",
    "        target_next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        model_loss = nn.MSELoss()(predicted_next_state, target_next_state)\n",
    "        model_loss.backward()\n",
    "        self.optimizer_model.step()\n",
    "\n",
    "    def plan(self, replay_buffer, num_steps):\n",
    "        for _ in range(num_steps):\n",
    "            state, action, reward, next_state, terminal = replay_buffer.sample()\n",
    "            self.update(state, action, reward, next_state, terminal)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_q_network.load_state_dict(self.q_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a7e8958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeeva/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m action \u001b[38;5;241m=\u001b[39m dyna_q_agent\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[1;32m     31\u001b[0m next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 32\u001b[0m dyna_q_agent\u001b[38;5;241m.\u001b[39mupdate(state, action, reward, next_state, done)\n\u001b[1;32m     33\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     34\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m, in \u001b[0;36mDynaQ.update\u001b[0;34m(self, state, action, reward, next_state, terminal)\u001b[0m\n\u001b[1;32m     23\u001b[0m next_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_q_network(torch\u001b[38;5;241m.\u001b[39mtensor(next_state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[1;32m     24\u001b[0m target_q \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m terminal) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(next_q_values)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 25\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()(q_values[action], target_q)\n\u001b[1;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_q\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmse_loss(\u001b[38;5;28minput\u001b[39m, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3328\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target):\n\u001b[1;32m   3325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   3326\u001b[0m         mse_loss, (\u001b[38;5;28minput\u001b[39m, target), \u001b[38;5;28minput\u001b[39m, target, size_average\u001b[38;5;241m=\u001b[39msize_average, reduce\u001b[38;5;241m=\u001b[39mreduce, reduction\u001b[38;5;241m=\u001b[39mreduction\n\u001b[1;32m   3327\u001b[0m     )\n\u001b[0;32m-> 3328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m   3329\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   3330\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis will likely lead to incorrect results due to broadcasting. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3333\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   3334\u001b[0m     )\n\u001b[1;32m   3335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "# Initialize DynaQ agent\n",
    "#env = Environment()  # Replace with your environment class\n",
    "#env_name = 'CartPole-v1'\n",
    "#env_name = 'MountainCarContinuous-v0'\n",
    "env_name = 'MountainCar-v0'\n",
    "#env_name = 'Pendulum-v1'\n",
    "\n",
    "# Create the CartPole environment\n",
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action = env.action_space.sample()\n",
    "if isinstance(action, int):\n",
    "    action_dim = env.observation_space.shape[0] + 1\n",
    "elif isinstance(action, np.ndarray):\n",
    "    action_dim = env.observation_space.shape[0] + len(action)\n",
    "#action_dim = len(env.action_space.sample())\n",
    "dyna_q_agent = DynaQ(state_dim, action_dim)\n",
    "\n",
    "num_episodes = 10\n",
    "max_episode_length = 200\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]\n",
    "    total_reward = 0\n",
    "    episode_length = 0\n",
    "    \n",
    "    while episode_length < max_episode_length:\n",
    "        action = dyna_q_agent.select_action(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        dyna_q_agent.update(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "                break\n",
    "        episode_length = episode_length + 1\n",
    "\n",
    "    dyna_q_agent.update_target_network()\n",
    "    dyna_q_agent.plan(replay_buffer, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6732d1a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 159\u001b[0m\n\u001b[1;32m    156\u001b[0m agent \u001b[38;5;241m=\u001b[39m DynaQ(state_dim, action_dim, hidden_dim, gamma, alpha, beta, buffer_size, batch_size)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain(train_loader, episodes)\n",
      "Cell \u001b[0;32mIn[4], line 104\u001b[0m, in \u001b[0;36mDynaQ.train\u001b[0;34m(self, train_loader, episodes)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_loader, episodes):\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n\u001b[0;32m--> 104\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (state, action, next_state, reward) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m    105\u001b[0m             state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    106\u001b[0m             action \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import deque\n",
    "\n",
    "# Define Gaussian Non-deterministic Neural Network for Environment Model\n",
    "class EnvironmentModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(EnvironmentModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, state_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, state_dim)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat((state, action), dim=-1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        next_state_mean = self.fc2(x)\n",
    "        next_state_stddev = torch.exp(self.fc3(x))\n",
    "        return next_state_mean, next_state_stddev\n",
    "\n",
    "\n",
    "# Define Normal Deterministic Neural Network for Policy Model\n",
    "class PolicyModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim):\n",
    "        super(PolicyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        action = torch.tanh(self.fc2(x))\n",
    "        return action\n",
    "\n",
    "\n",
    "# Dyna-Q Algorithm\n",
    "class DynaQ:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, gamma, alpha, beta, buffer_size, batch_size):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.env_model = EnvironmentModel(state_dim, action_dim, hidden_dim)\n",
    "        self.policy_model = PolicyModel(state_dim, action_dim, hidden_dim)\n",
    "        self.env_optimizer = optim.Adam(self.env_model.parameters(), lr=alpha)\n",
    "        self.policy_optimizer = optim.Adam(self.policy_model.parameters(), lr=beta)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        return self.policy_model(state).detach().numpy()\n",
    "\n",
    "    def update_model(self):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = np.random.choice(len(self.buffer), self.batch_size, replace=False)\n",
    "        state_batch, action_batch, next_state_batch, reward_batch = zip(*[self.buffer[i] for i in batch])\n",
    "        state_batch = torch.FloatTensor(state_batch)\n",
    "        action_batch = torch.FloatTensor(action_batch)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch)\n",
    "        reward_batch = torch.FloatTensor(reward_batch)\n",
    "\n",
    "        # Update policy model\n",
    "        predicted_action = self.policy_model(state_batch)\n",
    "        q_values = torch.sum(predicted_action * self.env_model(state_batch, predicted_action)[0], dim=1)\n",
    "        loss_policy = -torch.mean(q_values)\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        loss_policy.backward()\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "        # Update environment model\n",
    "        next_state_pred_mean, next_state_pred_stddev = self.env_model(state_batch, action_batch)\n",
    "        loss_env = nn.MSELoss()(next_state_pred_mean, next_state_batch) + torch.mean(next_state_pred_stddev)\n",
    "        self.env_optimizer.zero_grad()\n",
    "        loss_env.backward()\n",
    "        self.env_optimizer.step()\n",
    "\n",
    "    def update_q_values(self, state, action, next_state, reward):\n",
    "        state = torch.FloatTensor(state)\n",
    "        action = torch.FloatTensor(action)\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        reward = torch.FloatTensor(reward)\n",
    "\n",
    "        predicted_action = self.policy_model(state)\n",
    "        q_value = torch.sum(predicted_action * self.env_model(state, predicted_action)[0])\n",
    "\n",
    "        next_predicted_action = self.policy_model(next_state)\n",
    "        next_q_value = torch.sum(next_predicted_action * self.env_model(next_state, next_predicted_action)[0])\n",
    "\n",
    "        target = reward + self.gamma * next_q_value\n",
    "        loss_q = nn.MSELoss()(q_value, target.detach())\n",
    "\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        loss_q.backward()\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "    def train(self, train_loader, episodes):\n",
    "        for episode in range(episodes):\n",
    "            for batch_idx, (state, action, next_state, reward) in enumerate(train_loader):\n",
    "                state = state.float()\n",
    "                action = action.float()\n",
    "                next_state = next_state.float()\n",
    "                reward = reward.float()\n",
    "\n",
    "                self.buffer.append((state, action, next_state, reward))\n",
    "                self.update_model()\n",
    "\n",
    "                for i in range(self.batch_size):\n",
    "                    self.update_q_values(state[i], action[i], next_state[i], reward[i])\n",
    "\n",
    "            print(\"Episode:\", episode + 1)\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # Initialize CartPole environment\n",
    "    env_name = 'CartPole-v1'\n",
    "    #env_name = 'MountainCarContinuous-v0'\n",
    "    #env_name = 'MountainCar-v0'\n",
    "    #env_name = 'Pendulum-v1'\n",
    "\n",
    "    # Create the CartPole environment\n",
    "    #env = gym.make(env_name)\n",
    "    batch_size = 32\n",
    "\n",
    "    # Load datasets\n",
    "    with open(env_name + '_train_dataset.pkl', 'rb') as f:\n",
    "        train_dataset_loaded = pickle.load(f)\n",
    "\n",
    "    with open(env_name + '_test_dataset.pkl', 'rb') as f:\n",
    "        test_dataset_loaded = pickle.load(f)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset_loaded, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset_loaded, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Define environment parameters\n",
    "    state_dim = train_dataset_loaded[0][0].shape[0]\n",
    "    action_dim = train_dataset_loaded[0][1].shape[0]\n",
    "    hidden_dim = 32\n",
    "    gamma = 0.99\n",
    "    alpha = 0.001\n",
    "    beta = 0.001\n",
    "    buffer_size = 10000\n",
    "    episodes = 100\n",
    "\n",
    "    # Initialize Dyna-Q agent\n",
    "    agent = DynaQ(state_dim, action_dim, hidden_dim, gamma, alpha, beta, buffer_size, batch_size)\n",
    "\n",
    "    # Train the agent\n",
    "    agent.train(train_loader, episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98097c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
